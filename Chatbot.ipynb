{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manzoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manzoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manzoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wikipedia import page\n",
    "import random\n",
    "import string \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "from pandas import DataFrame\n",
    "import pyttsx3 \n",
    "import speech_recognition as sr\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Weather details for city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get('https://www.timeanddate.com/weather/india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def temp(topic):\n",
    "    \n",
    "    page = page1\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "    data = soup.find(class_ = 'zebra fw tb-wt zebra va-m')\n",
    "\n",
    "    tags = data('a')\n",
    "    city = [tag.contents[0] for tag in tags]\n",
    "    tags2 = data.find_all(class_ = 'rbi')\n",
    "    temp = [tag.contents[0] for tag in tags2]\n",
    "\n",
    "    indian_weather = pd.DataFrame(\n",
    "    {\n",
    "        'City':city,\n",
    "        'Temperature':temp\n",
    "    }\n",
    "    )\n",
    "    \n",
    "    df = indian_weather[indian_weather['City'].str.contains(topic.title())] \n",
    "    \n",
    "    return (df['Temperature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape city detail from Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_data(topic):\n",
    "    \n",
    "    topic=topic.title()\n",
    "    topic=topic.replace(' ', '_',1)\n",
    "    url1=\"https://en.wikipedia.org/wiki/\"\n",
    "    url=url1+topic\n",
    "\n",
    "    source = urllib.request.urlopen(url).read()\n",
    "\n",
    "    # Parsing the data/ creating BeautifulSoup object\n",
    "    soup = bs.BeautifulSoup(source,'lxml')\n",
    "\n",
    "    # Fetching the data\n",
    "    text = \"\"\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text += paragraph.text\n",
    "\n",
    "    import re\n",
    "    # Preprocessing the data\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    \n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Special char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sorry I dont understand you'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rem_special(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    return(text.translate(remove_punct_dict))\n",
    "\n",
    "sample_text=\"I am sorry! I don't understand you.\"\n",
    "rem_special(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is eat. He play yesterday. He will be go tomorrow.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def stemmer(text):\n",
    "    words = word_tokenize(text) \n",
    "    for w in words:\n",
    "        text=text.replace(w,PorterStemmer().stem(w))\n",
    "    return text\n",
    "\n",
    "stemmer(\"He is Eating. He played yesterday. He will be going tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock', 'corpus', 'better']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "sample_text=\"rocks corpora better\" #default noun\n",
    "LemTokens(nltk.word_tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample sentence , showing stop words filtration .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"This is a sample sentence, showing off the stop words filtration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding part of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "spacy_df=[]\n",
    "spacy_df1=[]\n",
    "df_spacy_nltk=pd.DataFrame()\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "  \n",
    "# Process whole documents \n",
    "sample_text = (\"The heavens are above. The moral code of conduct is above the civil code of conduct\") \n",
    "doc = nlp(sample_text) \n",
    "  \n",
    "# Token and Tag \n",
    "for token in doc:\n",
    "    spacy_df.append(token.pos_)\n",
    "    spacy_df1.append(token)\n",
    "\n",
    "\n",
    "df_spacy_nltk['origional']=spacy_df1\n",
    "df_spacy_nltk['spacy']=spacy_df\n",
    "#df_spacy_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1919 DATE\n",
      "Birmingham GPE\n",
      "England GPE\n",
      "Tommy Shelby PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "def ner(sentence):\n",
    "    doc = nlp(sentence) \n",
    "    for ent in doc.ents: \n",
    "        print(ent.text, ent.label_) \n",
    "    \n",
    "\n",
    "sentence = \"A gangster family epic set in 1919 Birmingham, England; centered on a gang who sew razor blades in the peaks of their caps, and their fierce boss Tommy Shelby.\"\n",
    "ner(sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity 0.7\n",
      "polarity -0.35\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def senti(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    return(testimonial.polarity)\n",
    "\n",
    "sample_text=\"This apple is good\"\n",
    "print(\"polarity\",senti(sample_text))\n",
    "sample_text=\"This apple is not good\"\n",
    "print(\"polarity\",senti(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happening elephant text lucknow sweet'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "def spelling(text):\n",
    "    splits = sample_text.split()\n",
    "    for split in splits:\n",
    "        text=text.replace(split,spell.correction(split))\n",
    "        \n",
    "    return (text)\n",
    "    \n",
    "    \n",
    "sample_text=\"hapenning elephnt texte luckno sweeto\"\n",
    "spelling(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey how are you?', 'I am fine.']\n",
      "['Hey', 'how', 'are', 'you', '?', 'I', 'am', 'fine', '.']\n"
     ]
    }
   ],
   "source": [
    "#TOkenisation\n",
    "print(nltk.sent_tokenize(\"Hey how are you? I am fine.\"))\n",
    "print(nltk.word_tokenize(\"Hey how are you? I am fine.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>is</th>\n",
       "      <th>messi</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      about        is     messi     tfidf      this\n",
       "0  0.448321  0.448321  0.630099  0.000000  0.448321\n",
       "1  0.448321  0.448321  0.000000  0.630099  0.448321"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documentA = 'This is about Messi'\n",
    "documentB = 'This is about TFIDF'\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice enabled\n",
    "### Chatbot speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(message):\n",
    "    engine= pyttsx3.init()\n",
    "    engine.say('{}'.format(message))\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init() \n",
    "\n",
    "engine.say(\"Hello hi\") \n",
    "engine.runAndWait() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "with mic as source:\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    audio = r.listen(source)\n",
    "text_audio=(r.recognize_google(audio))\n",
    "print(r.recognize_google(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.say(text_audio) \n",
    "engine.runAndWait() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionary for cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = {} \n",
    "city[\"lucknow\"] = [\"lucknow\", \"lko\"]\n",
    "city[\"delhi\"]=[\"new delhi\",'ndls']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_name(sentence):\n",
    "    for word in sentence.split():\n",
    "        for key, values in city.items():\n",
    "            \n",
    "            if word.lower() in values:\n",
    "                return(key)\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemNormalize(text):\n",
    "    text=rem_special(text) #remove special char\n",
    "    text=text.lower() # lower case\n",
    "    text=remove_stopwords(text) # remove stop words\n",
    "    \n",
    "    return LemTokens(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating answer using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating answer\n",
    "def response(user_input):\n",
    "    \n",
    "    ToGu_response=''\n",
    "    sent_tokens.append(user_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')   \n",
    "    all_word_vectors = word_vectorizer.fit_transform(sent_tokens)  \n",
    "    \n",
    "   \n",
    "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors) \n",
    "    idx=similar_vector_values.argsort()[0][-2]\n",
    "    \n",
    "\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "    \n",
    "    if(vector_matched==0):\n",
    "        ToGu_response=ToGu_response+\"I am sorry! I don't understand you.\"\n",
    "        return ToGu_response\n",
    "    else:\n",
    "        ToGu_response = ToGu_response+sent_tokens[idx]\n",
    "        return ToGu_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the city name you want to ask queries for: lko\n"
     ]
    }
   ],
   "source": [
    "topic=str(input(\"Please enter the city name you want to ask queries for: \"))\n",
    "topic=city_name(topic)\n",
    "text=wiki_data(topic)\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(text)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(text)# converts to list of words\n",
    "weather_reading=(temp(topic)).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greetings Keyword matching\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACES_INPUTS = (\"places\", \"monuments\", \"buildings\",\"places\", \"monument\", \"building\")\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "def ner(sentence):\n",
    "    places_imp=\"\"\n",
    "    doc = nlp(sentence) \n",
    "    for ent in doc.ents: \n",
    "        if (ent.label_==\"FAC\"):\n",
    "            #print(ent.text, ent.label_) \n",
    "            places_imp=places_imp+ent.text+\",\"+\" \"\n",
    "            \n",
    "    return(places_imp)\n",
    "    \n",
    "\n",
    "places_imp=ner(text) \n",
    "\n",
    "\n",
    "s=places_imp\n",
    "l = s.split() \n",
    "k = [] \n",
    "for i in l: \n",
    "  \n",
    "    # If condition is used to store unique string  \n",
    "    # in another list 'k'  \n",
    "    if (s.count(i)>1 and (i not in k)or s.count(i)==1): \n",
    "        k.append(i) \n",
    "\n",
    "PLACES_RESPONSES = ' '.join(k)\n",
    "\n",
    "def places(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in PLACES_INPUTS:\n",
    "            return (PLACES_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_INPUTS = (\"weather\", \"temp\", \"temperature\")\n",
    "\n",
    "WEATHER_RESPONSES =weather_reading\n",
    "\n",
    "def weather(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in WEATHER_INPUTS:\n",
    "            return (WEATHER_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToGu: Hello\n",
      "User:hi\n",
      "Sentiment score= 0.0\n",
      "ToGu: hello\n",
      "User:weather\n",
      "Sentiment score= 0.0\n",
      "ToGu: 33 °C\n",
      "User:places\n",
      "Sentiment score= 0.0\n",
      "ToGu: Important places are the bara imambara, chota mahatma gandhi road, charan singh international airport, metro, great mir, imambara nazim saheb, victoria street, shatranj ke khiladi, syed modi grand prix, ambedkar memorial,\n",
      "User:food\n",
      "Sentiment score= 0.0\n",
      "ToGu: lucknow is also known for its chaats, street food, kulfi, paan and sweets.\n",
      "User:bye\n",
      "Sentiment score= 0.0\n",
      "ToGu: Goodbye.\n"
     ]
    }
   ],
   "source": [
    "continue_dialogue=True\n",
    "print(\"ToGu: Hello\")\n",
    "speak(\"Hello\")\n",
    "\n",
    "while(continue_dialogue==True):\n",
    "    user_input = input(\"User:\")\n",
    "    user_input=user_input.lower()\n",
    "    user_input=spelling(user_input) #spelling check\n",
    "    print(\"Sentiment score=\",senti(user_input)) #sentiment score\n",
    "    \n",
    "    if(user_input!='bye'):\n",
    "        if(user_input=='thanks' or user_input=='thank you' ):\n",
    "            print(\"ToGu: You are welcome..\")\n",
    "            speak(\" You are welcome\")\n",
    "            \n",
    "        else:\n",
    "            if(greeting(user_input)!=None):\n",
    "                tmp=greeting(user_input)\n",
    "                print(\"ToGu: \"+tmp)\n",
    "                speak(tmp)\n",
    "                \n",
    "            elif(weather(user_input)!=None):\n",
    "                tmp=weather(user_input)\n",
    "                print(\"ToGu: \"+tmp)\n",
    "                speak(tmp)\n",
    "                \n",
    "                \n",
    "            elif(places(user_input)!=None):\n",
    "                tmp=places(user_input)\n",
    "                print(\"ToGu: Important places are \"+tmp)\n",
    "                speak(\"Important places are\")\n",
    "                speak(tmp)\n",
    "                \n",
    "            else:\n",
    "                print(\"ToGu: \",end=\"\")\n",
    "                temp=response(user_input)\n",
    "                print(temp) \n",
    "                speak(temp)\n",
    "                sent_tokens.remove(user_input)\n",
    "                \n",
    "\n",
    "    else:\n",
    "        continue_dialogue=False\n",
    "        print(\"ToGu: Goodbye.\")\n",
    "        speak(\"goodbye\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
